{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd53eacf",
   "metadata": {},
   "source": [
    "这里说频率是指一个机场航班出现的频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bayes-opt-xgb-2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:07.569305Z",
     "start_time": "2025-10-19T12:58:06.418820Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用贝叶斯优化替代上方的手动超参数训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgb_bayes_opt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bayesian_tune_xgb, smape_eval\n\u001b[1;32m      4\u001b[0m base_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 上文已定义 dtrain/dval/dtest，使用验证集作为优化目标\u001b[39;00m\n",
      "File \u001b[0;32m~/hh_1/xgb_bayes_opt.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Tuple\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# pip install bayesian-optimization\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayes_opt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianOptimization\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/xgboost/core.py:58\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     _T,\n\u001b[1;32m     39\u001b[0m     ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     c_bst_ulong,\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PANDAS_INSTALLED, DataFrame, import_cupy, py_str\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_lib_path\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mXGBoostError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/xgboost/compat.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# pandas\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, MultiIndex, Series\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat \u001b[38;5;28;01mas\u001b[39;00m pandas_concat\n\u001b[1;32m     37\u001b[0m     PANDAS_INSTALLED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/compat/__init__.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# 使用贝叶斯优化替代上方的手动超参数训练\n",
    "from xgb_bayes_opt import bayesian_tune_xgb, smape_eval\n",
    "\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "\n",
    "# 上文已定义 dtrain/dval/dtest，使用验证集作为优化目标\n",
    "evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "model_bayes, best_params, best_smape = bayesian_tune_xgb(\n",
    "    dtrain,\n",
    "    evals,\n",
    "    base_params=base_params,\n",
    "    init_points=8,\n",
    "    n_iter=25,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10,\n",
    "    save_model_name='xgb_model_bayes.json',\n",
    ")\n",
    "\n",
    "print('Best SMAPE:', best_smape)\n",
    "print('Best Params:', best_params)\n",
    "\n",
    "# 使用最优模型进行测试集预测\n",
    "y_pred_bayes = model_bayes.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bayes-opt-xgb-1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:07.574624Z",
     "start_time": "2025-10-19T12:58:07.574611Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用贝叶斯优化进行XGBoost超参数调优\n",
    "# 需要安装: pip install bayesian-optimization\n",
    "from xgb_bayes_opt import bayesian_tune_xgb, smape_eval\n",
    "\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "\n",
    "# 使用验证集作为优化目标\n",
    "evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "model_bayes, best_params, best_smape = bayesian_tune_xgb(\n",
    "    dtrain,\n",
    "    evals,\n",
    "    base_params=base_params,\n",
    "    init_points=8,\n",
    "    n_iter=25,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10,\n",
    "    save_model_name='xgb_model_bayes.json',\n",
    ")\n",
    "\n",
    "print('Best SMAPE:', best_smape)\n",
    "print('Best Params:', best_params)\n",
    "\n",
    "# 使用最优模型进行测试集预测\n",
    "y_pred_bayes = model_bayes.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682e039",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba85eca",
   "metadata": {},
   "source": [
    "数据已经预处理过了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67758591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T13:00:59.245350Z",
     "start_time": "2025-10-19T13:00:59.138278Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:07.309304Z",
     "iopub.status.busy": "2025-06-16T09:13:07.308981Z",
     "iopub.status.idle": "2025-06-16T09:13:12.472460Z",
     "shell.execute_reply": "2025-06-16T09:13:12.471269Z",
     "shell.execute_reply.started": "2025-06-16T09:13:07.309273Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/compat/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     ISMUSL,\n\u001b[1;32m     20\u001b[0m     PY310,\n\u001b[1;32m     21\u001b[0m     PY311,\n\u001b[1;32m     22\u001b[0m     PY312,\n\u001b[1;32m     23\u001b[0m     PYPY,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ISMUSL' from 'pandas.compat._constants' (/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/compat/_constants.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 加载数据\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_hh/result/pre_2023-2025_with_comp_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflt_no\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m})\n",
      "File \u001b[0;32m/data/zy/anaconda3/envs/machine/lib/python3.10/site-packages/pandas/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     get_option,\n\u001b[1;32m     39\u001b[0m     set_option,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     options,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('data_hh/result/pre_2023-2025_with_comp_train.csv', dtype={'flt_no': str})\n",
    "\n",
    "# 查看前几行数据，确保加载成功\n",
    "# 显示前两行数据以确保正确加载\n",
    "print(data.shape)\n",
    "print(data.head(5))\n",
    "print(data.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21010a-ed73-426d-ad74-292659c6d09b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.548386Z",
     "start_time": "2025-10-19T12:58:23.548372Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:12.473938Z",
     "iopub.status.busy": "2025-06-16T09:13:12.473627Z",
     "iopub.status.idle": "2025-06-16T09:13:12.534223Z",
     "shell.execute_reply": "2025-06-16T09:13:12.533151Z",
     "shell.execute_reply.started": "2025-06-16T09:13:12.473906Z"
    }
   },
   "outputs": [],
   "source": [
    "# 检查标准化后的统计信息\n",
    "print(\"\\n标准化后的统计信息：\")\n",
    "print(data['pax'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070bc17",
   "metadata": {},
   "source": [
    "## 编码分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85031f",
   "metadata": {},
   "source": [
    "### 新增城市标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70a5b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.549185Z",
     "start_time": "2025-10-19T12:58:23.549173Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:12.537491Z",
     "iopub.status.busy": "2025-06-16T09:13:12.536767Z",
     "iopub.status.idle": "2025-06-16T09:13:12.543501Z",
     "shell.execute_reply": "2025-06-16T09:13:12.542458Z",
     "shell.execute_reply.started": "2025-06-16T09:13:12.537454Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# 加载字典\n",
    "with open('data_hh/result/encoder/city_labels_航班频率加权图标签.json', 'r') as file:\n",
    "    city_labels_loaded = json.load(file)\n",
    "\n",
    "print(\"加载的字典：\", city_labels_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf2ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.549947Z",
     "start_time": "2025-10-19T12:58:23.549935Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:12.545593Z",
     "iopub.status.busy": "2025-06-16T09:13:12.544633Z",
     "iopub.status.idle": "2025-06-16T09:13:13.055097Z",
     "shell.execute_reply": "2025-06-16T09:13:13.053925Z",
     "shell.execute_reply.started": "2025-06-16T09:13:12.545553Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用 map 对 'a', 'b', 'c', 'from', 'to' 列进行标签化，新增对应的标签列\n",
    "data['a_label'] = data['a'].map(city_labels_loaded)\n",
    "data['b_label'] = data['b'].map(city_labels_loaded)\n",
    "data['c_label'] = data['c'].map(city_labels_loaded)\n",
    "data['from_label'] = data['from'].map(city_labels_loaded)\n",
    "data['to_label'] = data['to'].map(city_labels_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23199201",
   "metadata": {},
   "source": [
    "### 新增城市二维嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9a4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.550973Z",
     "start_time": "2025-10-19T12:58:23.550960Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:13.056939Z",
     "iopub.status.busy": "2025-06-16T09:13:13.056599Z",
     "iopub.status.idle": "2025-06-16T09:13:13.064523Z",
     "shell.execute_reply": "2025-06-16T09:13:13.063375Z",
     "shell.execute_reply.started": "2025-06-16T09:13:13.056897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加载字典\n",
    "with open('data_hh/result/encoder/城市嵌入编码_航班频率加权图.json', 'r') as file:\n",
    "    city_embeddings = json.load(file)\n",
    "\n",
    "print(\"加载的字典：\", city_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92374fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.552253Z",
     "start_time": "2025-10-19T12:58:23.552241Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:13.066397Z",
     "iopub.status.busy": "2025-06-16T09:13:13.066071Z",
     "iopub.status.idle": "2025-06-16T09:13:16.452194Z",
     "shell.execute_reply": "2025-06-16T09:13:16.451227Z",
     "shell.execute_reply.started": "2025-06-16T09:13:13.066365Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 将 city_embeddings 转换为 DataFrame\n",
    "embedding_df = pd.DataFrame.from_dict(city_embeddings, orient='index', columns=['embedding_1', 'embedding_2'])\n",
    "embedding_df.index.name = 'city'\n",
    "\n",
    "# 用 'a', 'b', 'c', 'from', 'to' 字段与 embedding_df 合并\n",
    "data = data.merge(embedding_df, left_on='a', right_index=True, how='left')\n",
    "data.rename(columns={'embedding_1': 'a_embedding_1', 'embedding_2': 'a_embedding_2'}, inplace=True)\n",
    "\n",
    "data = data.merge(embedding_df, left_on='b', right_index=True, how='left')\n",
    "data.rename(columns={'embedding_1': 'b_embedding_1', 'embedding_2': 'b_embedding_2'}, inplace=True)\n",
    "\n",
    "data = data.merge(embedding_df, left_on='c', right_index=True, how='left')\n",
    "data.rename(columns={'embedding_1': 'c_embedding_1', 'embedding_2': 'c_embedding_2'}, inplace=True)\n",
    "\n",
    "data = data.merge(embedding_df, left_on='from', right_index=True, how='left')\n",
    "data.rename(columns={'embedding_1': 'from_embedding_1', 'embedding_2': 'from_embedding_2'}, inplace=True)\n",
    "\n",
    "data = data.merge(embedding_df, left_on='to', right_index=True, how='left')\n",
    "data.rename(columns={'embedding_1': 'to_embedding_1', 'embedding_2': 'to_embedding_2'}, inplace=True)\n",
    "\n",
    "# 查看添加的新列\n",
    "print(data[['a_embedding_1', 'a_embedding_2', 'b_embedding_1', 'b_embedding_2', 'c_embedding_1', 'c_embedding_2', 'from_embedding_1', 'from_embedding_2', 'to_embedding_1', 'to_embedding_2']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9be248",
   "metadata": {},
   "source": [
    "### 频率编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2737257",
   "metadata": {},
   "source": [
    "使用 json 保存和加载 city_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c9648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.553215Z",
     "start_time": "2025-10-19T12:58:23.553203Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:16.453820Z",
     "iopub.status.busy": "2025-06-16T09:13:16.453489Z",
     "iopub.status.idle": "2025-06-16T09:13:17.488226Z",
     "shell.execute_reply": "2025-06-16T09:13:17.487071Z",
     "shell.execute_reply.started": "2025-06-16T09:13:16.453788Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载city_map\n",
    "with open('data_hh/result/encoder/city_map_频率编码.json', 'r') as f:\n",
    "    city_map = json.load(f)\n",
    "\n",
    "# 使用 city_map 替换指定列的值\n",
    "columns_to_replace = ['a', 'b', 'c', 'from', 'to']\n",
    "\n",
    "# 遍历指定列并直接用 map 映射\n",
    "for col in columns_to_replace:\n",
    "    data[col] = data[col].map(city_map)\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3b1ea-44b4-4d7f-9c6e-7036e2ee1fc6",
   "metadata": {},
   "source": [
    "### 'flt_no', 'bd_type', 'aircraft'编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022bd85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.554187Z",
     "start_time": "2025-10-19T12:58:23.554175Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:17.489916Z",
     "iopub.status.busy": "2025-06-16T09:13:17.489583Z",
     "iopub.status.idle": "2025-06-16T09:13:18.769790Z",
     "shell.execute_reply": "2025-06-16T09:13:18.768611Z",
     "shell.execute_reply.started": "2025-06-16T09:13:17.489883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# 定义需要编码的分类特征\n",
    "# categorical_columns = ['flt_no', 'bd_type', 'aircraft']\n",
    "categorical_columns = ['flt_no', 'aircraft']\n",
    "\n",
    "# 从保存的文件中加载编码器并应用到data\n",
    "for col in categorical_columns:\n",
    "    # 加载编码器\n",
    "    encoder_path = os.path.join('data_hh/result/encoder/', f\"{col}_encoder_all.pkl\")\n",
    "    le = joblib.load(encoder_path)\n",
    "    \n",
    "    try:\n",
    "        # 对data进行转换\n",
    "        data[col] = le.transform(data[col])\n",
    "        print(f\"{col}列编码完成\")\n",
    "    except ValueError as e:\n",
    "        # 如果遇到新的类别，打印错误信息\n",
    "        print(f\"{col}列编码出错: {str(e)}\")\n",
    "        # 找出新的类别\n",
    "        new_categories = set(data[col]) - set(le.classes_)\n",
    "        print(f\"{col}列中的新类别: {new_categories}\")\n",
    "\n",
    "# 查看编码后的结果\n",
    "print(\"\\n编码后的前几行数据：\")\n",
    "print(data[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033293da",
   "metadata": {},
   "source": [
    "## 特征和目标分离\n",
    "我们要预测的是pax字段，其他字段作为特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a274b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.555202Z",
     "start_time": "2025-10-19T12:58:23.555190Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:18.771947Z",
     "iopub.status.busy": "2025-06-16T09:13:18.771085Z",
     "iopub.status.idle": "2025-06-16T09:13:18.967789Z",
     "shell.execute_reply": "2025-06-16T09:13:18.966562Z",
     "shell.execute_reply.started": "2025-06-16T09:13:18.771911Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征列\n",
    "# X = data[['flt_no', 'bd_type', 'cap', 'aircraft',  'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday','holiday', 'hour', 'minute', 'second', 'from', 'to','unit_price']]\n",
    "# X = data[['flt_no', 'bd_type', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday','hour', 'minute', 'second', 'from', 'to','unit_price']]\n",
    "# X = data[['flt_no', 'bd_type', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday','hour', 'minute', 'second', 'from', 'to','unit_price','a_label' ,'b_label' ,'c_label' ,'from_label' ,'to_label']]\n",
    "\n",
    "# 有abc，有标签，有嵌入\n",
    "# X = data[['flt_no', 'bd_type', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday','hour', 'minute', 'second', 'from', 'to','unit_price','a_label' ,'b_label' ,'c_label' ,'from_label' ,'to_label','a_embedding_1' , 'a_embedding_2' , 'b_embedding_1','b_embedding_2' , 'c_embedding_1' , 'c_embedding_2' , 'from_embedding_1','from_embedding_2' , 'to_embedding_1' , 'to_embedding_2']]\n",
    "# X = data[['flt_no', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday','hour', 'minute', 'from', 'to','unit_price','a_label' ,'b_label' ,'c_label' ,'from_label' ,'to_label','a_embedding_1' , 'a_embedding_2' , 'b_embedding_1','b_embedding_2' , 'c_embedding_1' , 'c_embedding_2' , 'from_embedding_1','from_embedding_2' , 'to_embedding_1' , 'to_embedding_2']]\n",
    "import numpy as np\n",
    "# 存在性检查→缺则补（只在缺失时计算）\n",
    "_need = ['quarter','is_weekend','is_holiday_season','hour_sin','hour_cos','month_sin','month_cos']\n",
    "_missing = [c for c in _need if c not in data.columns]\n",
    "if _missing:\n",
    "    assert all(col in data.columns for col in ['month','weekday','hour']), '缺少 month/weekday/hour 无法补时间特征'\n",
    "    data['quarter'] = data['month'].apply(lambda x: (x - 1) // 3 + 1)\n",
    "    data['is_weekend'] = data['weekday'].isin([5, 6]).astype(int)\n",
    "    data['is_holiday_season'] = data['month'].isin([1, 2, 7, 8, 10]).astype(int)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "X = data[['flt_no', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'a', 'b', 'c', 'year', 'month', 'day', 'weekday', 'hour', 'minute', 'quarter', 'is_weekend', 'is_holiday_season', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'from', 'to', 'unit_price', 'competitor_price', 'a_label', 'b_label', 'c_label', 'from_label', 'to_label', 'a_embedding_1', 'a_embedding_2', 'b_embedding_1', 'b_embedding_2', 'c_embedding_1', 'c_embedding_2', 'from_embedding_1', 'from_embedding_2', 'to_embedding_1', 'to_embedding_2']]\n",
    "\n",
    "# 删除了abc，但有标签，有嵌入\n",
    "# X = data[['flt_no', 'bd_type', 'cap', 'aircraft', 'legs', 'leg_no', 'duration', 'year', 'month', 'day', 'weekday','hour', 'minute', 'second', 'unit_price','a_label' ,'b_label' ,'c_label' ,'from_label' ,'to_label','a_embedding_1' , 'a_embedding_2' , 'b_embedding_1','b_embedding_2' , 'c_embedding_1' , 'c_embedding_2' , 'from_embedding_1','from_embedding_2' , 'to_embedding_1' , 'to_embedding_2']]\n",
    "\n",
    "# 目标列\n",
    "y = data['pax']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37ae67",
   "metadata": {},
   "source": [
    "### 对x进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef4398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.556828Z",
     "start_time": "2025-10-19T12:58:23.556816Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:18.969569Z",
     "iopub.status.busy": "2025-06-16T09:13:18.969193Z",
     "iopub.status.idle": "2025-06-16T09:13:21.448820Z",
     "shell.execute_reply": "2025-06-16T09:13:21.447833Z",
     "shell.execute_reply.started": "2025-06-16T09:13:18.969536Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 对所有特征进行标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 将标准化后的数据转回 DataFrame 格式\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# 查看结果\n",
    "pd.set_option('display.max_columns', None)  # 显示所有列\n",
    "print(X_scaled.head(1))\n",
    "\n",
    "\n",
    "# 保存 scaler\n",
    "joblib.dump(scaler, 'data_hh/result/encoder/standard_scaler_x.pkl')\n",
    "print(\"x的标准化器已保存为 standard_scaler_x.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af705c8",
   "metadata": {},
   "source": [
    "### 对y进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6228c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.557678Z",
     "start_time": "2025-10-19T12:58:23.557665Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:21.450358Z",
     "iopub.status.busy": "2025-06-16T09:13:21.450043Z",
     "iopub.status.idle": "2025-06-16T09:13:21.483423Z",
     "shell.execute_reply": "2025-06-16T09:13:21.482453Z",
     "shell.execute_reply.started": "2025-06-16T09:13:21.450328Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对目标列 y 进行标准化\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))  # 将 y 转换为 2D 数组进行标准化\n",
    "\n",
    "# 转换回 DataFrame 格式\n",
    "y_scaled = pd.DataFrame(y_scaled, columns=['pax_scaled'])\n",
    "\n",
    "# 查看标准化后的 y\n",
    "print(y_scaled.head())\n",
    "\n",
    "# 保存 y 的 scaler\n",
    "joblib.dump(scaler_y, 'data_hh/result/encoder/standard_scaler_y.pkl')\n",
    "print(\"y的标准化器已保存为 standard_scaler_y.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e832ee",
   "metadata": {},
   "source": [
    "## 训练XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aabaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.558470Z",
     "start_time": "2025-10-19T12:58:23.558457Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:13:21.487432Z",
     "iopub.status.busy": "2025-06-16T09:13:21.487087Z",
     "iopub.status.idle": "2025-06-16T09:15:11.497986Z",
     "shell.execute_reply": "2025-06-16T09:15:11.496985Z",
     "shell.execute_reply.started": "2025-06-16T09:13:21.487380Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 自定义 SMAPE 函数\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    return np.mean(diff / denominator) * 100\n",
    "\n",
    "# 自定义评估函数\n",
    "def smape_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    smape_value = smape(y_true, y_pred)\n",
    "    return 'SMAPE', smape_value\n",
    "\n",
    "# 数据划分\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 输出数据集大小\n",
    "print(f'训练集大小: {X_train.shape[0]}')\n",
    "print(f'验证集大小: {X_val.shape[0]}')\n",
    "print(f'测试集大小: {X_test.shape[0]}')\n",
    "\n",
    "# 转换为 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 使用贝叶斯优化替代手动参数设置\n",
    "from xgb_bayes_opt import bayesian_tune_xgb, smape_eval\n",
    "\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    # 可选: 'tree_method': 'hist', 'nthread': 8, 'seed': 42\n",
    "}\n",
    "\n",
    "# 评估集（以验证集为优化目标）\n",
    "evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "model, best_params, best_smape = bayesian_tune_xgb(\n",
    "    dtrain,\n",
    "    evals,\n",
    "    base_params=base_params,\n",
    "    init_points=8,\n",
    "    n_iter=25,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10,\n",
    "    save_model_name='xgb_model_bayes.json',\n",
    ")\n",
    "\n",
    "print('Best SMAPE:', best_smape)\n",
    "print('Best Params:', best_params)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# # 测试集 SMAPE 评估\n",
    "# test_smape = smape(y_test, y_pred)\n",
    "# print(f'SMAPE on Test Set: {test_smape:.2f}%')\n",
    "\n",
    "# # 测试集 MSE 评估\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f'Mean Squared Error on Test Set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d952ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.559619Z",
     "start_time": "2025-10-19T12:58:23.559607Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.499446Z",
     "iopub.status.busy": "2025-06-16T09:15:11.499011Z",
     "iopub.status.idle": "2025-06-16T09:15:11.509419Z",
     "shell.execute_reply": "2025-06-16T09:15:11.508425Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.499383Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5b420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.560479Z",
     "start_time": "2025-10-19T12:58:23.560467Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.510923Z",
     "iopub.status.busy": "2025-06-16T09:15:11.510554Z",
     "iopub.status.idle": "2025-06-16T09:15:11.517389Z",
     "shell.execute_reply": "2025-06-16T09:15:11.516458Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.510889Z"
    }
   },
   "outputs": [],
   "source": [
    "# 反标准化 y_test\n",
    "y_test_original = scaler_y.inverse_transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# 反标准化预测结果 y_pred\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb350a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.561385Z",
     "start_time": "2025-10-19T12:58:23.561373Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.518997Z",
     "iopub.status.busy": "2025-06-16T09:15:11.518611Z",
     "iopub.status.idle": "2025-06-16T09:15:11.525200Z",
     "shell.execute_reply": "2025-06-16T09:15:11.524127Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.518965Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3699b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.562126Z",
     "start_time": "2025-10-19T12:58:23.562114Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.526975Z",
     "iopub.status.busy": "2025-06-16T09:15:11.526295Z",
     "iopub.status.idle": "2025-06-16T09:15:11.532762Z",
     "shell.execute_reply": "2025-06-16T09:15:11.531674Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.526942Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5b674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.563011Z",
     "start_time": "2025-10-19T12:58:23.562998Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.534214Z",
     "iopub.status.busy": "2025-06-16T09:15:11.533871Z",
     "iopub.status.idle": "2025-06-16T09:15:11.541960Z",
     "shell.execute_reply": "2025-06-16T09:15:11.540880Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.534183Z"
    }
   },
   "outputs": [],
   "source": [
    "# 正确的写法\n",
    "test_results = list(zip(y_test_original[:100], y_pred_original[:100]))  # 真实值和预测值\n",
    "\n",
    "print(\"\\n20条测试结果（真实值 vs 预测值）:\")\n",
    "for i, (true_value, pred_value) in enumerate(test_results[:40]):\n",
    "    # 如果是多维数组，使用 .item() 转换为标量\n",
    "    true_value = true_value.item() if isinstance(true_value, np.ndarray) else true_value\n",
    "    pred_value = pred_value.item() if isinstance(pred_value, np.ndarray) else pred_value\n",
    "    print(f\"第{i+1}条: 真实值={true_value}, 预测值={pred_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e830ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.564429Z",
     "start_time": "2025-10-19T12:58:23.564417Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.543450Z",
     "iopub.status.busy": "2025-06-16T09:15:11.543098Z",
     "iopub.status.idle": "2025-06-16T09:15:11.564448Z",
     "shell.execute_reply": "2025-06-16T09:15:11.563454Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.543384Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true).ravel(), np.array(y_pred).ravel()\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_pred - y_true)\n",
    "    \n",
    "    # 避免除以零，将分母中为零的项替换为一个小值\n",
    "    denominator = np.where(denominator == 0, 1e-8, denominator)\n",
    "    \n",
    "    smape = 100 * np.mean(diff / denominator)\n",
    "    return smape\n",
    "\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 Mean Absolute Percentage Error (MAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true).ravel(), np.array(y_pred).ravel()\n",
    "    \n",
    "    # 避免除以零，将 y_true 中的零值替换为一个小值\n",
    "    y_true = np.where(y_true == 0, 1e-8, y_true)\n",
    "    \n",
    "    mape = 100 * np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "    return mape\n",
    "\n",
    "\n",
    "# 假设 y_test 和 y_pred 已经是标准化反归一化后的数据\n",
    "# 将其转换为一维数组以确保形状一致\n",
    "y_test = np.array(y_test_original).ravel()\n",
    "y_pred = np.array(y_pred_original).ravel()\n",
    "\n",
    "# 评估指标\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = calculate_mape(y_test, y_pred)\n",
    "smape = calculate_smape(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# 打印结果\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape:.4f}%')\n",
    "print(f'Symmetric Mean Absolute Percentage Error (SMAPE): {smape:.4f}%')\n",
    "print(f'R-squared (R²): {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc958d",
   "metadata": {},
   "source": [
    "似乎对于较小值预测存在误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4598a3",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd242864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.565388Z",
     "start_time": "2025-10-19T12:58:23.565376Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:11.566447Z",
     "iopub.status.busy": "2025-06-16T09:15:11.565602Z",
     "iopub.status.idle": "2025-06-16T09:15:13.012522Z",
     "shell.execute_reply": "2025-06-16T09:15:13.011474Z",
     "shell.execute_reply.started": "2025-06-16T09:15:11.566392Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_model(\"data_hh/result/model/归一化_xgboost_model_1000.json\")\n",
    "print(\"模型已保存为 xgboost_model_1000.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd202e",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb90e9",
   "metadata": {},
   "source": [
    "## 不同特征重要程度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5789db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T12:58:23.566649Z",
     "start_time": "2025-10-19T12:58:23.566637Z"
    },
    "execution": {
     "iopub.execute_input": "2025-06-16T09:15:13.014139Z",
     "iopub.status.busy": "2025-06-16T09:15:13.013810Z",
     "iopub.status.idle": "2025-06-16T09:15:13.018532Z",
     "shell.execute_reply": "2025-06-16T09:15:13.017579Z",
     "shell.execute_reply.started": "2025-06-16T09:15:13.014108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 model 是训练好的 XGBoost 模型\n",
    "xgb.plot_importance(model, importance_type='weight', title=\"Feature Importance (Weight)\", height=0.5)\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_importance(model, importance_type='gain', title=\"Feature Importance (Gain)\", height=0.5)\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_importance(model, importance_type='cover', title=\"Feature Importance (Cover)\", height=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe396b1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "753.984px",
    "left": "22px",
    "top": "111.141px",
    "width": "377.641px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
